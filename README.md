<p align="center">
    <h1 align="center">Seminar Project: NLP Model for detecting offensive language in Football Comments</h1>
</p>
<p align="center">
  <img src="https://github.com/prizyou/NLP4B_Football/raw/main/Stock_Title_Picture.jpeg" alt="ProjectStockPic">
</p>
<p align="center">
    <em>Unlocking Football Insights! </em>
</p>
<p align="center">
	<img src="https://img.shields.io/badge/Python-3.10-blue" alt="Python">
	<img src="https://img.shields.io/badge/License-MIT-orange" alt="License">
<p>
<p align="center">
		<em>Developed with the Software / Tools below.</em>
</p>
<p align="center">
	<img src="https://img.shields.io/badge/Jupyter-F37626.svg?style=flat&logo=Jupyter&logoColor=white" alt="Jupyter">
	<img src="https://img.shields.io/badge/Python-3776AB.svg?style=flat&logo=Python&logoColor=white" alt="Python">
</p>
<hr>

##  Quick Links

> - [Overview](#overview)
> - [Project Roadmap](#project-roadmap)
> - [Features](#features)
> - [Repository Structure](#repository-structure)
> - [Getting Started](#getting-started)
>   - [Installation](#installation)
>   - [Running NLP4B_Football](#running-nlp4b_football)
>   - [Tests](#tests)
> - [License](#license)

---

##  Overview

NLP4B_Football is a Natural Language Processing project focused on football data, leveraging functionalities like data cleaning, normalization, and analysis powered by BERT and an Ordinary Logistics Regression Model. It tokenizes emojis into text, enhancing readability, and uses machine learning models for insightful prediction, making football data more accessible and analyzable.

---

##  Project Roadmap

### Preperation Phase

- [X] `â–º Find Data Sources`
- [X] `â–º Install Anaconda`
- [X] `â–º Join the Githup repository`
- [X] `â–º Collect Ideas for the Project`

### Main Phase

- [X] `â–º Collect Data from Instagram / Reddit`
- [X] `â–º Collect Data from Kaggle Datasaet and Clean Data`
- [X] `â–º Think of how to merge the datasets`
- [X] `â–º Clean and Label the Data`
- [X] `â–º Research about BERT`
- [X] `â–º Implement the BERT Model`
- [X] `â–º Brainstorm a second Model and implement it`
- [X] `â–º Study code, pose questions, refine algorithm`
- [X] `â–º Finish BERT Transformation and generate Outpout`
- [X] `â–º Perform model validation and conformance checking`
- [X] `â–º Apply cross validation and overfitting check`
- [X] `â–º Use the NLP Models on the Youtube Dataset to See weather League or International is more offensive`

### Presentaion Phase

- [X] `â–º Work on the Project Report parts`
- [X] `â–º Prepare the Presentaion of the Project`

---

##  Features

|    |   Feature         | Description |
|----|-------------------|---------------------------------------------------------------|
| âš™ï¸  | **Architecture** | This project mainly revolves around NLP tasks using a BERT model and a custom model, with distinct sections for data cleaning, normalization, and making predictions.|
| ğŸ”© | **Code Quality**  | The code is modular and organized in different directories based on functionality.|
| ğŸ“„ | **Documentation** | This README provides an overview of the Project Flow, as well as the [PROJECT REPORT](https://github.com/prizyou/NLP4B_Football/blob/main/Project%20Report.pdf).|
| ğŸ”Œ | **Integrations**  | This project pulls data from different sources like Reddit and YouTube.|
| ğŸ§© | **Modularity**    | Code is structured in different modules based on functionality (Data cleaning, model creation, prediction etc.).|
| âš¡ï¸  | **Performance**  | Both Models perform well with Scores at minimum over 80, up to 96 in all common metrices (Accuracy, Recall, ...).|
| ğŸ“¦ | **Dependencies**  | Major dependencies include Python, Jupyter Notebooks, and various NLP Python libraries for BERT and the custom model.|


---

##  Repository Structure

```sh
â””â”€â”€ NLP4B_Football/
    â”‚
    â”‚
    â”œâ”€â”€ BERT_model
    â”‚   â”‚
    â”‚   â”œâ”€â”€ data_normalization.py
    â”‚   â”œâ”€â”€ labeled_data.xlsx
    â”‚   â”œâ”€â”€ labeled_normalized_data.csv
    â”‚   â””â”€â”€ model.ipynb
    â”‚
    â”‚
    â”œâ”€â”€ Own_model
    â”‚   â”‚
    â”‚   â”œâ”€â”€ model creation
    â”‚   â”‚   â”œâ”€â”€ complete_dataset.csv
    â”‚   â”‚   â”œâ”€â”€ logreg_model.pkl
    â”‚   â”‚   â”œâ”€â”€ own_model_creation.ipynb
    â”‚   â”‚   â”œâ”€â”€ pred_youtube_cleaned_for_label.xlsx
    â”‚   â”‚   â”œâ”€â”€ vectorizer.pkl
    â”‚   â”‚   â””â”€â”€ youtube_cleaned_for_label_NEW.xlsx
    â”‚   â”‚
    â”‚   â””â”€â”€ model_prediction
    â”‚       â”œâ”€â”€ logreg_model.pkl
    â”‚       â”œâ”€â”€ own_model_prediction.ipynb
    â”‚       â””â”€â”€ vectorizer.pkl
    â”‚
    â”‚
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Stock_Title_Picture.jpeg
    â”‚
    â”‚
    â”œâ”€â”€ data_cleaning
    â”‚   â”‚
    â”‚   â”œâ”€â”€ clean_not_labeled_data
    â”‚   â”‚   â”œâ”€â”€ convert.ipynb
    â”‚   â”‚   â”œâ”€â”€ reddit_clean_not_labeled.csv
    â”‚   â”‚   â”œâ”€â”€ reddit_clean_not_labeled.xlsx
    â”‚   â”‚   â”œâ”€â”€ reddit_cleaned_for_label.xlsx
    â”‚   â”‚   â””â”€â”€ reddit_cleaning.py
    â”‚   â”‚
    â”‚   â”œâ”€â”€ kaggle
    â”‚   â”‚   â”œâ”€â”€ Kaggle_clean_and_labeled.xlsx
    â”‚   â”‚   â”œâ”€â”€ convert.py
    â”‚   â”‚   â”œâ”€â”€ kaggle_clean_and_labeled.csv
    â”‚   â”‚   â””â”€â”€ kaggle_cleaning.py
    â”‚   â”‚
    â”‚   â”œâ”€â”€ reddit
    â”‚   â”‚   â”œâ”€â”€ reddit_cleaned_for_label_NEW.xlsx
    â”‚   â”‚   â””â”€â”€ reddit_cleaning.ipynb
    â”‚   â”‚
    â”‚   â””â”€â”€ youtube
    â”‚       â”œâ”€â”€ youtube_cleaned_for_label.xlsx
    â”‚       â”œâ”€â”€ youtube_cleaned_for_label_NEW.xlsx
    â”‚       â””â”€â”€ yt_cleaning.ipynb
    â”‚
    â”‚
    â”œâ”€â”€ main
    â”‚   â””â”€â”€ main.ipynb
    â”‚
    â”‚
    â”œâ”€â”€ data_analysis
    â”‚   â”œâ”€â”€ complete_dataset.csv
    â”‚   â”œâ”€â”€ complete_dataset.xlsx
    â”‚   â””â”€â”€ dataset_analysis.ipynb
    â”‚
    â”‚
    â””â”€â”€ web_scraping
        â”‚
        â”œâ”€â”€ NOT_USED
        â”‚   â”‚
        â”‚   â”œâ”€â”€ facebook_OUT
        â”‚   â”‚   â”œâ”€â”€ __pycache__
        â”‚   â”‚   â”‚   â””â”€â”€ secrets.cpython-310.pyc
        â”‚   â”‚   â”œâ”€â”€ facebook_scrape.ipynb
        â”‚   â”‚   â”œâ”€â”€ new_facebook_scraper.ipynb
        â”‚   â”‚   â””â”€â”€ secrets.py
        â”‚   â”‚
        â”‚   â”œâ”€â”€ instagram_OUT
        â”‚   â”‚   â””â”€â”€ insta_scrape.py
        â”‚   â”‚
        â”‚   â””â”€â”€ reddit_OUT
        â”‚       â”œâ”€â”€ r_football_Paul.csv
        â”‚       â”œâ”€â”€ redditScrape_Ali.ipynb
        â”‚       â””â”€â”€ reddit_api_Paul.ipynb
        â”‚
        â”‚
        â”œâ”€â”€ reddit
        â”‚   â”œâ”€â”€ Reddit_IDs.txt
        â”‚   â”œâ”€â”€ pw.txt
        â”‚   â”œâ”€â”€ redditComments_Ali.csv
        â”‚   â”œâ”€â”€ redditComments_NEW.csv
        â”‚   â””â”€â”€ scraping.ipynb
        â”‚
	â”‚
        â””â”€â”€ youtube
            â”œâ”€â”€ dataScrapeFromYoutube.ipynb
            â”œâ”€â”€ videos_used.txt
            â”œâ”€â”€ yt_comments.csv
            â””â”€â”€ yt_comments_new.csv
```

---

##  Getting Started

***Requirements***

Ensure you have the following dependencies installed on your system:

* **JupyterNotebook**: `version x.y.z`

###  Installation

1. Clone the NLP4B_Football repository:

```sh
git clone https://github.com/prizyou/NLP4B_Football
```

2. Change to the project directory:

```sh
cd NLP4B_Football
```

3. Install the dependencies:

```sh
pip install -r requirements.txt
```

###  Running NLP4B_Football

Use the following command to run NLP4B_Football:

```sh
jupyter nbconvert --execute notebook.ipynb
```

###  Tests

To execute tests, run:

```sh
pytest notebook_test.py
```

---

##  License

This project is protected under the MIT License. For more details, refer to the [LICENSE](https://github.com/prizyou/NLP4B_Football/blob/main/LICENSE) file.
