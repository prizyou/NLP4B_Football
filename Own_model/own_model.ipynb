{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general packages for data manipulation\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#consistent sized plot \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize']=12,5\n",
    "rcParams['axes.labelsize']=12\n",
    "rcParams['xtick.labelsize']=12\n",
    "rcParams['ytick.labelsize']=12\n",
    "#handle the warnings in the code\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "#text preprocessing libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "#import texthero\n",
    "#import texthero as hero\n",
    "#regular expressions\n",
    "import re\n",
    "#display pandas dataframe columns \n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load csv file as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  source  label\n",
      "0  As a woman you shouldn't complain about cleani...  kaggle      0\n",
      "1  boy dats cold...tyga dwn bad for cuffin dat ho...  kaggle      1\n",
      "2  Dawg!!!! You ever fuck a bitch and she start t...  kaggle      1\n",
      "3  The shit you hear about me might be true or it...  kaggle      1\n",
      "4  The shit just blows me..claim you so faithful ...  kaggle      1\n",
      "(2398, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r'C:\\VSCode\\NLP4B_Football\\Own_model\\labeled_normalized_data.csv')\n",
    "\n",
    "#drop the first column because it is not necessary\n",
    "data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "#copy to a new dataframe\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find things to remove and how often they appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items found: 5 ['@soccerboy_04', '@bluprint_4', '@KingCuh', '@WestYourMajesty', '@']\n",
      "Total items found: 49 ['#Shots', '#2MW', '#HappyHumpDay', '#Eaglesnation', '#EarlyChristmas', '#CowboysNation', '#TehGodClan', '#Yankees', '#FreeMoneyMelle', '#oomf', '#scally', '#fixed', '#KingOfTheHill', '#UCFPINKPARTY', '#bum', '#hoesaintloyal', '#real', '#Pisces', '#GerrysHalloweenParty', '#MTVHottest', '#Yankees', '#ProtectTheAnimals', '#Damn', '#', '#blondeproblems', '#scally', '#History', '#frenchscally', '#YoureNotMyType', '#shitmybosssays', '#shitallysays', '#FreshRhymes', '#128514', '#JT2020Tour', '#50centmovie', '#ThankYouPaulForConfirmingLarry', '#NottingHill', '#8230', '#233', '#8230', '#RIPTALLT', '#65292', '#Tupac', '#afterearth', '#SNL', '#hoes', '#ShitFahdSays', '#redskins', '#1']\n",
      "Total items found: 2 ['https://x.com/dfb', 'https://youtu.be/8dIQ56YACvE']\n",
      "Total items found: 1829 [\"'\", '.', '!', '.', '.', ':', '.', '?', '.', \"'\", \"'\", '.', '#', '#', ',', '#', \"'\", '\"', '.', \"'\", '\"', '\"', '\"', \"'\", '?', '-', '-', '?', '.', '\"', '?', '#', ',', '!', \"'\", '\"', '/', '.', '!', '.', ',', '\"', '.', '\"', '\"', '\"', '!', \"'\", '#', '.', '\"', \"'\", '!', '\"', ',', '.', ',', '.', '\"', \"'\", '?', ':', '\"', \"'\", '.', \"'\", ',', \"'\", \"'\", '*', '.', ',', \"'\", '\"', '\"', '!', \"'\", '|', \"'\", '?', '\"', '!', '!', '.', \"'\", \"'\", \"'\", \"'\", '\"', ',', '\"', \"'\", '#', '.', '#', '.', '.', '\"', '.', \"'\", '\"', \"'\", '\"', \"'\", '.', \"'\", \"'\", '#', \"'\", ',', \"'\", '\"', '.', ',', '.', '?', '\"', '\"', '-', '\"', '\"', '\"', \"'\", '.', '/', '.', '\"', '.', '?', '?', \"'\", \"'\", '\"', '\"', '\"', \"'\", '\"', '.', \"'\", '!', \"'\", '.', \"'\", ':', '%', \"'\", '/', '!', \"'\", '\"', '#', '\"', '.', '\"', '!', '.', \"'\", \"'\", '@', '.', '\"', '-', '.', '?', \"'\", '\"', '#', '@', '\"', \"'\", '\"', '\"', \"'\", '\"', ',', '#', '\"', '-', '\"', \"'\", '\"', '\"', ',', '\"', \"'\", '.', \"'\", '\"', '-', \"'\", '\"', '\"', '\"', '.', '.', \"'\", \"'\", '\"', \"'\", '.', '\"', '\"', '.', '.', '\"', '\"', ',', '.', '?', \"'\", ',', \"'\", ',', '.', \"'\", '.', '?', '\"', '\"', '.', ',', ',', ',', '.', '@', '.', '#', '-', '.', '\"', '\"', ',', '\"', '.', '.', '\"', '#', '!', ':', \"'\", '!', \"'\", '\"', \"'\", '!', '?', \"'\", ',', \"'\", '.', \"'\", \"'\", \"'\", '\"', '\"', '\"', '\"', '?', '!', '=', '!', '!', '!', '!', \"'\", '!', \"'\", '\"', '.', \"'\", '.', '!', '\"', '.', '#', \"'\", '?', \"'\", \"'\", \"'\", '\"', ',', \"'\", ',', '.', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', ':', '.', \"'\", '-', '.', '.', ':', '…', ':', '.', '’', \"'\", '.', \"'\", '.', \"'\", '.', ':', '!', '.', \"'\", '.', '…', ':', ':', '.', '\"', '’', \"'\", '’', ':', '.', ':', \"'\", ':', '’', '’', ':', ':', '.', '.', '’', ':', ':', '?', '/', '.', '-', \"'\", \"'\", '.', '.', ':', '.', '!', \"'\", '.', ':', '’', \"'\", '.', '?', '.', ':', '.', \"'\", \"'\", ':', '.', \"'\", ':', '?', '(', \"'\", '=', '.', '’', ':', '.', '?', '.', '.', '.', ':', ':', ':', '’', '.', '-', ':', '.', '’', ':', '.', \"'\", \"'\", \"'\", '.', '!', '’', '.', ':', '“', '.', '.', ':', '<', '.', ':', ':', ':', '.', ':', '<', '?', '.', '!', '!', '.', '.', '.', '’', '.', '?', '!', ':', '!', \"'\", \"'\", '!', '!', '!', ':', '!', '.', \"'\", '.', '.', '?', ':', \"'\", '!', '!', '!', '.', '’', \"'\", '.', '.', '.', ':', '.', '!', ':', ':', \"'\", '.', '-', '?', '.', '’', '!', '.', \"'\", ':', '’', '?', ':', ':', '’', \"'\", \"'\", \"'\", '-', '.', '?', '.', '’', '’', '’', '’', \"'\", \"'\", \"'\", '.', '.', '.', '.', '?', '.', ':', '.', \"'\", '-', '.', ':', ':', '.', '.', '?', '?', '?', '!', '<', \"'\", '.', ':', '’', ':', \"'\", '-', '!', '.', \"'\", '.', '!', '.', '“', '’', '?', '.', '’', ':', ':', '!', '.', \"'\", ':', '.', '.', ':', '&', '.', '.', ':', ':', '/', ':', '…', ':', '’', '.', '*', '<', \"'\", \"'\", ':', ':', '…', '.', '’', ':', '.', '!', '.', '.', '.', '’', '.', '.', '.', \"'\", '-', '?', '.', ':', \"'\", '.', '.', '”', '.', '?', '?', '!', '’', '\"', \"'\", ':', ':', \"'\", ':', ':', ':', '’', '.', '.', '\"', ':', ':', \"'\", '!', '’', '!', ':', '.', ':', ':', ':', '.', ':', '.', ':', ':', ':', ':', ':', ':', '.', ':', '.', '&', ':', '=', ':', \"'\", ':', ':', '!', '.', '.', ':', '.', \"'\", ':', '.', '.', '!', \"'\", \"'\", ':', ':', ':', ':', ':', ':', '?', '.', ':', ':', ':', ':', \"'\", '-', ':', '.', ':', ':', \"'\", '?', ':', '?', '.', ':', '!', '.', '?', '.', ':', \"'\", '.', '\"', '!', ':', '.', \"'\", '.', '.', '’', \"'\", \"'\", '.', '.', '.', \"'\", \"'\", '’', '.', '!', '.', ':', '-', '?', \"'\", '.', '\"', '.', '.', '.', '’', '.', '-', \"'\", '.', '’', '“', '.', '.', '.', '’', '.', '!', '&', '.', '.', '‘', \"'\", '.', '!', '.', '#', '.', '?', '’', ':', '.', '.', '-', ':', \"'\", '’', \"'\", '…', ':', '<', \"'\", ':', ':', '?', '.', '.', '.', '.', ':', '.', \"'\", \"'\", \"'\", '’', ':', \"'\", '.', ':', '.', '’', '.', ':', '.', '.', '.', '.', \"'\", ':', \"'\", '/', '‘', ':', '.', '.', \"'\", \"'\", ':', '.', ':', '.', ':', '.', ':', '.', '!', ':', ':', ':', '’', '!', '?', '.', '.', '’', '-', '!', '?', '’', '‘', '’', ':', '-', \"'\", '?', ':', \"'\", \"'\", '.', \"'\", '-', '.', ':', \"'\", '.', \"'\", '.', ':', '-', ':', '.', '’', '.', '?', '.', ':', \"'\", \"'\", '.', \"'\", '’', '’', ':', '!', '&', \"'\", ':', '.', \"'\", '.', ':', '(', ':', \"'\", \"'\", '?', '.', \"'\", '.', '.', '\"', '.', '!', '.', \"'\", ':', '?', '?', ':', '-', ':', \"'\", \"'\", '-', '-', '?', ':', '£', '’', '!', ':', '.', \"'\", '.', ':', '.', '.', '.', '.', \"'\", '’', \"'\", '’', '/', '?', '.', '.', '.', '.', '-', ':', '.', '.', '’', \"'\", ':', '.', '’', '.', '…', '’', '.', '.', '.', ':', '’', '?', '.', '.', ':', \"'\", ':', '.', '-', '.', \"'\", '?', '.', '.', '?', '.', '!', '’', ':', '.', '.', '-', \"'\", ':', '.', '.', ':', '?', \"'\", '?', '!', '?', ':', \"'\", ':', '.', '.', '’', '?', '?', ':', \"'\", '.', ':', '!', '!', '.', '’', ':', '*', '?', '.', ':', '.', ':', ':', '?', '.', '’', '.', '?', '.', '.', \"'\", '&', '.', '.', '?', '.', '.', '…', '’', ':', '.', '’', '.', ':', '…', '.', '.', \"'\", '?', '.', '.', ':', ':', ':', '.', '-', '-', '-', \"'\", '\"', '’', '.', ':', '’', '.', \"'\", '.', ':', ':', ':', ':', '.', '!', '’', '.', ':', \"'\", '-', '-', '.', '.', '’', '.', '.', '?', '.', ':', ':', ':', '!', '’', '!', '.', ':', ':', \"'\", \"'\", '.', '.', ':', ':', ':', '.', '.', ':', '’', '.', ':', '.', '.', ':', \"'\", '£', \"'\", '-', '!', \"'\", ':', '!', '!', '’', '.', ':', '\"', ':', '‘', '.', '?', ':', ':', ':', ':', '.', ':', '.', ';', '’', '.', '’', ':', '.', '.', '!', '’', '.', '.', ':', '’', '/', ':', '.', '’', '.', ':', ':', '.', '?', '‘', \"'\", '!', '’', '.', \"'\", '.', '.', '!', '-', '.', ':', ':', '’', '!', '-', '.', '’', \"'\", '.', '‘', '.', '?', \"'\", '!', '-', '-', '.', \"'\", ':', '.', \"'\", ':', ':', '?', ':', ':', '-', ':', '’', '?', ':', ':', '-', \"'\", '-', '.', \"'\", '.', '.', '.', '.', '.', ':', ':', '.', '.', '.', '-', ':', '.', '.', '.', '.', '?', ':', \"'\", ':', '’', '!', ':', '.', '.', '…', ':', ':', '-', ':', '\"', \"'\", '/', '.', '.', \"'\", '’', '!', '.', \"'\", \"'\", '.', '.', '.', '.', '’', ':', \"'\", '.', '.', \"'\", '.', '!', '.', \"'\", \"'\", ':', '.', '?', '.', '.', '.', '!', '-', '?', '.', '?', '.', '’', \"'\", '.', '.', '.', '!', '(', ':', '.', ':', ':', '!', '.', ':', ':', ':', '-', '!', \"'\", '!', '!', ':', \"'\", \"'\", \"'\", '.', '<', '.', ':', '.', ':', '(', ':', ':', '!', ':', '\"', '…', \"'\", ':', ':', '.', '.', '.', ':', ':', '<', '.', '.', '.', ':', '.', ':', \"'\", '?', ':', '.', ':', ':', ':', ':', \"'\", '.', ':', '.', ':', '(', \"'\", '’', ':', '/', '’', ':', '.', ':', \"'\", \"'\", '!', ':', \"'\", '.', \"'\", '.', '!', ':', '.', ':', '.', \"'\", '’', '.', ':', \"'\", '<', '.', '’', ':', '.', '’', ',', '’', ',', \"'\", '’', ',', '-', ',', ',', '’', \"'\", '’', ',', '’', ',', ',', '.', '.', '.', '.', '.', '.', '.', \"'\", ':', ',', '…', '?', ',', '/', '?', '.', '.', ',', '-', '?', \"'\", '.', ',', '.', '!', ',', '’', \"'\", ',', ':', '-', '.', '…', '.', '.', ':', ',', '.', ',', ',', '.', '.', '?', '’', '?', '.', ',', '.', '?', ',', ':', ',', \"'\", ',', '’', '.', '(', '.', '!', '.', ',', '?', ',', '?', \"'\", ',', '.', '!', ',', '.', '.', \"'\", \"'\", '\\\\', '?', ',', ',', '.', \"'\", '?', '’', ',', '?', \"'\", ',', '-', '?', '’', '’', '’', ',', '-', '.', '+', '.', '.', ',', ',', ',', ',', '.', '.', '.', ',', \"'\", \"'\", '<', ':', \"'\", \"'\", ',', '\"', '-', '.', ',', '.', ',', '.', '*', '-', '’', ',', \"'\", '?', '.', '?', '.', \"'\", \"'\", '.', ',', '.', \"'\", ':', '.', ',', '-', '.', \"'\", ',', '.', '.', \"'\", ',', '.', '.', \"'\", '.', '/', \"'\", \"'\", '?', '.', ',', '’', '’', '’', ',', ',', ',', ',', '?', ',', '.', '-', '?', '’', '’', \"'\", ',', '/', \"'\", \"'\", ',', ',', ',', \"'\", '.', '.', \"'\", '-', '\"', ':', '.', ',', '-', '.', '-', \"'\", '-', \"'\", '’', '.', \"'\", '&', '’', ',', \"'\", '’', '’', '(', '.', '?', '’', '.', \"'\", '?', '’', '?', '!', '?', '.', \"'\", \"'\", ',', '-', \"'\", \"'\", '.', '.', \"'\", '?', '.', ',', '.', \"'\", \"'\", '?', '.', \"'\", '.', \"'\", '’', '-', ':', ',', \"'\", ',', '.', \"'\", '’', '?', '’', '.', ',', '!', '.', \"'\", ',', ':', '?', \"'\", '?', '?', \"'\", \"'\", '.', '.', \"'\", '’', '?', \"'\", '?', '?', ',', \"'\", '?', '?', '.', '-', '.', ',', \"'\", \"'\", '.', '.', \"'\", '.', '.', ',', '’', '.', '’', '.', '-', '.', '(', '.', \"'\", '.', '.', ',', \"'\", '.', '.', ',', '.', '.', '.', '.', \"'\", '.', ',', '.', ',', '-', '/', '.', '.', '.', '’', '’', '.', '“', ',', '(', '.', \"'\", '.', '.', '.', ',', ',', \"'\", ',', '.']\n",
      "Total items found: 10 ['88', '88', '18', '18', '18', '18', '18', '88', '18', '18']\n",
      "['88', '88', '18', '18', '18', '18', '18', '88', '18', '18']\n"
     ]
    }
   ],
   "source": [
    "# Function to check for pattern\n",
    "def check_for_pattern(regex, dataframe, column_name):\n",
    "    '''Function to check for how often a pattern appears in a dataframe column and returns a list of all the items found'''\n",
    "    pattern = re.compile(regex)\n",
    "    result = []\n",
    "    for i in range(len(dataframe[column_name])):\n",
    "        phrase = (re.findall(pattern, dataframe[column_name][i]))\n",
    "        if phrase != []:\n",
    "            result.append(phrase[0])\n",
    "    print(\"Total items found:\", len(result), result)\n",
    "    return result\n",
    "\n",
    "# Check for user handles\n",
    "user_handles = check_for_pattern(r'@[\\w]*', df, 'comment')\n",
    "\n",
    "# Check for hashtags\n",
    "hashtags = check_for_pattern(r'#[\\w]*', df, 'comment')\n",
    "\n",
    "# Check for URLs\n",
    "urls = check_for_pattern(r'https?://[A-Za-z0-9./]+', df, 'comment')\n",
    "\n",
    "# Check for punctuations\n",
    "punctuations = check_for_pattern(r'[^\\w\\s]', df, 'comment')\n",
    "\n",
    "# Check for numbers 18, 88, 1312\n",
    "numbers = check_for_pattern(r'18|88|1312', df, 'comment')\n",
    "#print rows with numbers 18, 88, 1312, show only the comment column and show the whole comment\n",
    "print(numbers)\n",
    "# --> no need to worry\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove these patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>I meanhow good is Bellingham Crazy watching hi...</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>Stop being a pussy son and shove that needle i...</td>\n",
       "      <td>kaggle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>Unbelievable penalty given How on earth did th...</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>Id like to reaffirm that Rice has been the be...</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>How often does Darren Fletcher say And theres ...</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment   source  label\n",
       "1347  I meanhow good is Bellingham Crazy watching hi...  youtube      0\n",
       "468   Stop being a pussy son and shove that needle i...   kaggle      1\n",
       "1462  Unbelievable penalty given How on earth did th...  youtube      0\n",
       "2265   Id like to reaffirm that Rice has been the be...   reddit      0\n",
       "943   How often does Darren Fletcher say And theres ...  youtube      0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Remove user handles\n",
    "df.replace(r'@[\\w]*', '', regex=True, inplace=True)\n",
    "\n",
    "# Remove hashtags\n",
    "df.replace(r'#[\\w]*', '', regex=True, inplace=True)\n",
    "\n",
    "# Remove URLs\n",
    "df.replace(r'https?://[A-Za-z0-9./]+', '', regex=True, inplace=True)\n",
    "\n",
    "# Remove punctuations\n",
    "df.replace(r'[^\\w\\s]', '', regex=True, inplace=True)\n",
    "\n",
    "# Remove digits\n",
    "df.replace(r'\\d+', '', regex=True, inplace=True)\n",
    "\n",
    "\n",
    "# show random 5 rows\n",
    "df.sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decapitalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comment\"] = df[\"comment\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>[shouldve, played, at, least, one, academy, pl...</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>[i, know, utd, blundered, a, goal, lead, but, ...</td>\n",
       "      <td>youtube</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>[a, lot, of, nigeria, fans, at, wembley]</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>[welbeck, and, ali, should, be, in, nigeria, t...</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>[awakenbeerus, is, a, true, sports, savant, hi...</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment   source  label\n",
       "1649  [shouldve, played, at, least, one, academy, pl...  youtube      0\n",
       "1387  [i, know, utd, blundered, a, goal, lead, but, ...  youtube      1\n",
       "1766           [a, lot, of, nigeria, fans, at, wembley]  youtube      0\n",
       "1805  [welbeck, and, ali, should, be, in, nigeria, t...  youtube      0\n",
       "1861  [awakenbeerus, is, a, true, sports, savant, hi...  youtube      0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize using Tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "df['comment'] = df['comment'].apply(tokenizer.tokenize)\n",
    "\n",
    "#show random 5 rows\n",
    "df.sample(5, random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>[meanhow, good, bellingham, crazy, watching, l...</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>[stop, pussy, son, shove, needle, heart]</td>\n",
       "      <td>kaggle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>[unbelievable, penalty, given, earth, ref, give]</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>[id, like, reaffirm, rice, best, player, pitch]</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>[often, darren, fletcher, say, theres, chance]</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment   source  label\n",
       "1347  [meanhow, good, bellingham, crazy, watching, l...  youtube      0\n",
       "468            [stop, pussy, son, shove, needle, heart]   kaggle      1\n",
       "1462   [unbelievable, penalty, given, earth, ref, give]  youtube      0\n",
       "2265    [id, like, reaffirm, rice, best, player, pitch]   reddit      0\n",
       "943      [often, darren, fletcher, say, theres, chance]  youtube      0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#remove stopwords\n",
    "#nltk.download()\n",
    "stopwords = stopwords.words('english')\n",
    "df['comment'] = df['comment'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "\n",
    "#show random 5 rows\n",
    "df.sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spelling Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m corrected_text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Apply the correction function to the 'comment' column\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_spelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\pandas\\core\\series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\pandas\\core\\apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\pandas\\core\\apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[66], line 10\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m corrected_text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Apply the correction function to the 'comment' column\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m tokens: \u001b[43mcorrect_spelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[66], line 6\u001b[0m, in \u001b[0;36mcorrect_spelling\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect_spelling\u001b[39m(tokens):\n\u001b[0;32m      5\u001b[0m     textblob \u001b[38;5;241m=\u001b[39m TextBlob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens))\n\u001b[1;32m----> 6\u001b[0m     corrected_text \u001b[38;5;241m=\u001b[39m \u001b[43mtextblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m corrected_text\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\blob.py:580\u001b[0m, in \u001b[0;36mBaseBlob.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    578\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    579\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (Word(w)\u001b[38;5;241m.\u001b[39mcorrect() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[1;32m--> 580\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\blob.py:579\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;66;03m# regex matches: word or punctuation or whitespace\u001b[39;00m\n\u001b[0;32m    578\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (\u001b[43mWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[0;32m    580\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\blob.py:127\u001b[0m, in \u001b[0;36mWord.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Word(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspellcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\blob.py:119\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspellcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\en\\__init__.py:123\u001b[0m, in \u001b[0;36msuggest\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuggest\u001b[39m(w):\n\u001b[0;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspelling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\_text.py:1398\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(w, \u001b[38;5;241m1.0\u001b[39m)] \u001b[38;5;66;03m# 1.5\u001b[39;00m\n\u001b[0;32m   1396\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known([w]) \\\n\u001b[0;32m   1397\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w)) \\\n\u001b[1;32m-> 1398\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m) \\\n\u001b[0;32m   1399\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m [w]\n\u001b[0;32m   1400\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0.0\u001b[39m), c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[0;32m   1401\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p, word \u001b[38;5;129;01min\u001b[39;00m candidates) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\_text.py:1375\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\_text.py:1375\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w) \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(e1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\_text.py:96\u001b[0m, in \u001b[0;36mlazydict.__contains__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__contains__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proth\\.conda\\envs\\NLP4B_Project\\lib\\site-packages\\textblob\\_text.py:87\u001b[0m, in \u001b[0;36mlazydict._lazy\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, types\u001b[38;5;241m.\u001b[39mMethodType(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mdict\u001b[39m, method), \u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# apply spelling corrections on dataframe\n",
    "def correct_spelling(tokens):\n",
    "    textblob = TextBlob(' '.join(tokens))\n",
    "    corrected_text = textblob.correct()\n",
    "    return corrected_text.split()\n",
    "\n",
    "# Apply the correction function to the 'comment' column\n",
    "df['comment'] = df['comment'].apply(lambda tokens: correct_spelling(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_nonalpha(text):\n",
    "    '''Function to remove the non-alphanumeric characters from the text'''\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'comment' column\n",
    "df['comment'] = df['comment'].apply(rem_nonalpha(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for data balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['label'])\n",
    "plt.title('Count of Hate vs Non Hate Tweet')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP4B_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
